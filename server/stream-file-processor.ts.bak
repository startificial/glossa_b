import fs from 'fs';
import path from 'path';
import { pipeline } from 'stream/promises';
import { promisify } from 'util';
import { createReadStream, createWriteStream } from 'fs';
import { GoogleGenerativeAI } from '@google/generative-ai';
import * as mammoth from 'mammoth';
import { logger } from './utils/logger';

const mkdir = promisify(fs.mkdir);
const unlink = promisify(fs.unlink);
const stat = promisify(fs.stat);
const readFile = promisify(fs.readFile);

// Initialize Google Generative AI with appropriate model
const API_KEY = process.env.GEMINI_API_KEY || process.env.GOOGLE_API_KEY || '';
const genAI = new GoogleGenerativeAI(API_KEY);

// Default generation config (similar to what's in gemini.ts)
const generationConfig = {
  temperature: 0.7,
  topP: 0.8,
  topK: 40,
  maxOutputTokens: 8192,
};

// Safety settings with proper enum values from Google Generative AI lib
const safetySettings = [
  {
    category: "HARM_CATEGORY_HARASSMENT" as any,
    threshold: "BLOCK_MEDIUM_AND_ABOVE" as any,
  },
  {
    category: "HARM_CATEGORY_HATE_SPEECH" as any,
    threshold: "BLOCK_MEDIUM_AND_ABOVE" as any,
  },
  {
    category: "HARM_CATEGORY_SEXUALLY_EXPLICIT" as any,
    threshold: "BLOCK_MEDIUM_AND_ABOVE" as any,
  },
  {
    category: "HARM_CATEGORY_DANGEROUS_CONTENT" as any,
    threshold: "BLOCK_MEDIUM_AND_ABOVE" as any,
  },
];

/**
 * Process a file using efficient streaming techniques to avoid memory exhaustion
 * @param filePath Path to the uploaded file
 * @param fileName Original file name
 * @param projectName Name of the project
 * @param contentType Type of content (workflow, documentation, etc.)
 * @param fileType Type of file (document, pdf, image, etc.)
 * @param numAnalyses Number of analysis perspectives
 * @param reqPerAnalysis Number of requirements per analysis
 * @param inputDataId Optional ID for the input data record
 * @returns Array of generated requirements
 */
/**
 * Process a DOCX document and analyze its content
 * @param filePath Path to the DOCX file
 * @returns Object containing extracted text and analysis results
 */
export async function analyzeDocx(filePath: string): Promise<any> {
  try {
    logger.info(`Analyzing DOCX file: ${filePath}`);
    
    // Extract text from the document
    const textResult = await extractTextFromDocx(filePath);
    
    if (!textResult.success) {
      logger.error(`Failed to extract text from DOCX: ${textResult.error}`);
      return {
        success: false,
        error: textResult.error,
        metadata: {},
        context: {
          domain: "unknown",
          docType: "DOCX document",
          keywords: [],
          hasRequirements: false
        }
      };
    }
    
    const text = textResult.text;
    
    // Simplified metadata extraction - in a more complex implementation, 
    // we could use mammoth to extract more document properties
    const metadata = {
      textLength: text.length,
      format: "DOCX",
      processingTime: new Date().toISOString()
    };
    
    // Basic context detection - this could be enhanced with NLP or AI
    const keywords = text.split(/\s+/)
      .filter(word => word.length > 5)
      .filter((word, index, self) => self.indexOf(word) === index)
      .slice(0, 20);
    
    // Check if it likely contains requirements based on keyword detection
    const requirementsKeywords = ['shall', 'must', 'required', 'requirement', 'should', 'necessary'];
    // Add null/undefined check before using toLowerCase
    const hasRequirements = text && typeof text === 'string' 
      ? requirementsKeywords.some(kw => text.toLowerCase().includes(kw))
      : false;
    
    // Infer domain from content with null/undefined safety
    let domain = "general";
    if (!text || typeof text !== 'string') {
      domain = "unknown";
    } else if (text.toLowerCase().includes("software") || text.toLowerCase().includes("application")) {
      domain = "software";
    } else if (text.toLowerCase().includes("service") || text.toLowerCase().includes("customer")) {
      domain = "service management";
    } else if (text.toLowerCase().includes("sales") || text.toLowerCase().includes("marketing")) {
      domain = "sales and marketing";
    }
    
    return {
      success: true,
      text,
      metadata,
      context: {
        domain,
        docType: "DOCX document",
        keywords,
        hasRequirements
      }
    };
  } catch (error) {
    logger.error(`Error analyzing DOCX file: ${error}`);
    return {
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error analyzing DOCX',
      metadata: {},
      context: {
        domain: "unknown",
        docType: "DOCX document",
        keywords: [],
        hasRequirements: false
      }
    };
  }
}

/**
 * Extract text from a DOCX file
 * @param filePath Path to the DOCX file
 * @returns Object containing the extracted text and processing info
 */
export async function extractTextFromDocx(filePath: string): Promise<{text: string, success: boolean, error?: string}> {
  try {
    logger.info(`Extracting text from DOCX: ${filePath}`);
    
    // Check if the file exists
    if (!fs.existsSync(filePath)) {
      logger.error(`DOCX file not found: ${filePath}`);
      return {
        text: '',
        success: false,
        error: 'File not found'
      };
    }
    
    // Read the file buffer
    const buffer = await readFile(filePath);
    
    // Extract text from the document
    const result = await mammoth.extractRawText({ buffer });
    const text = result.value;
    
    // Log any warnings
    if (result.messages && result.messages.length > 0) {
      logger.warn(`Warnings while extracting text from DOCX: ${JSON.stringify(result.messages)}`);
    }
    
    if (!text || text.trim().length === 0) {
      logger.error('No text extracted from DOCX file');
      return {
        text: '',
        success: false,
        error: 'No text could be extracted from the document'
      };
    }
    
    logger.info(`Successfully extracted ${text.length} characters from DOCX`);
    return {
      text,
      success: true
    };
  } catch (error) {
    logger.error(`Error extracting text from DOCX file: ${error}`);
    return {
      text: '',
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error during DOCX text extraction'
    };
  }
}

/**
 * Extract text content from a text file
 * Uses a memory-efficient streaming approach for large files
 * 
 * @param filePath Path to the text file
 * @param streamMode Boolean to indicate if streaming mode should be used (for very large files)
 * @returns Object containing the extracted text and processing info
 */
export async function extractTextFromTxt(
  filePath: string, 
  streamMode: boolean = false
): Promise<{text: string, success: boolean, error?: string, preview?: string}> {
  try {
    logger.info(`Reading text from file: ${filePath}`);
    
    // Check if the file exists
    if (!fs.existsSync(filePath)) {
      logger.error(`Text file not found: ${filePath}`);
      return {
        text: '',
        success: false,
        error: 'File not found'
      };
    }
    
    // Get file stats to determine size
    const stats = fs.statSync(filePath);
    const fileSizeMB = stats.size / (1024 * 1024);
    
    // For large files, only read a preview in initial mode or use streaming
    if (fileSizeMB > 5 && streamMode) {
      logger.info(`Large text file detected (${fileSizeMB.toFixed(2)} MB). Using streaming mode.`);
      
      // For streaming mode, we don't load the full text, just return success
      // The actual content will be processed in chunks later
      return {
        text: '', // Not loading the actual text in streaming mode
        success: true,
        preview: await getTextFilePreview(filePath, 5000) // Get a small preview
      };
    }
    
    // For smaller files or when explicitly requesting full content, read the entire file
    const text = await readFile(filePath, 'utf8');
    
    if (!text || text.trim().length === 0) {
      logger.error('Text file is empty');
      return {
        text: '',
        success: false,
        error: 'Text file is empty'
      };
    }
    
    logger.info(`Successfully read ${text.length} characters from text file`);
    return {
      text,
      success: true
    };
  } catch (error) {
    logger.error(`Error reading text file: ${error}`);
    return {
      text: '',
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error during text file reading'
    };
  }
}

/**
 * Get a preview of a text file without loading the entire file
 * 
 * @param filePath Path to the text file
 * @param maxChars Maximum number of characters to read
 * @returns Preview text from the beginning of the file
 */
async function getTextFilePreview(filePath: string, maxChars: number = 5000): Promise<string> {
  try {
    // Create a read stream for the file
    const stream = fs.createReadStream(filePath, {
      encoding: 'utf8',
      start: 0,
      end: maxChars - 1 // Read only the first maxChars characters
    });
    
    let preview = '';
    
    // Process the stream in chunks
    for await (const chunk of stream) {
      preview += chunk;
      
      // If we've read enough, stop
      if (preview.length >= maxChars) {
        break;
      }
    }
    
    return preview;
  } catch (error) {
    logger.error(`Error getting text file preview: ${error}`);
    return 'Error getting preview';
  }
}

/**
 * Get file content based on file type 
 * @param filePath Path to the file
 * @param fileType Type of file (.pdf, .docx, .txt, etc)
 */
export async function getFileContent(filePath: string, fileType: string): Promise<{text: string, success: boolean, error?: string}> {
  try {
    logger.info(`Getting content from file: ${filePath} (type: ${fileType})`);
    
    // Check if file exists
    if (!fs.existsSync(filePath)) {
      logger.error(`File not found: ${filePath}`);
      return { text: '', success: false, error: 'File not found' };
    }
    
    // Process based on file type
    if (fileType.toLowerCase() === '.docx' || fileType.toLowerCase() === '.doc') {
      return await extractTextFromDocx(filePath);
    } else if (fileType.toLowerCase() === '.txt' || fileType.toLowerCase() === '.md') {
      return await extractTextFromTxt(filePath);
    } else {
      logger.error(`Unsupported file type for text extraction: ${fileType}`);
      return { 
        text: '', 
        success: false, 
        error: `Unsupported file type: ${fileType}. Supported types include .docx, .doc, .txt, and .md` 
      };
    }
  } catch (error) {
    logger.error(`Error getting file content: ${error}`);
    return {
      text: '',
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error extracting file content'
    };
  }
}

export async function streamProcessFile(
  filePath: string,
  fileName: string,
  projectName: string,
  contentType: string = 'general',
  fileType: string = 'other',
  numAnalyses: number = 2,
  reqPerAnalysis: number = 5,
  inputDataId?: number
): Promise<any[]> {
  // Create a temporary processing directory with a random ID
  const tempDir = path.join(path.dirname(filePath), `proc_${generateUniqueId(8)}`);
  try {
    // Create temporary directory for processing chunks
    await mkdir(tempDir, { recursive: true });
    
    // Get file information
    const stats = await stat(filePath);
    const fileSize = stats.size;
    const fileSizeInMB = fileSize / (1024 * 1024);
    
    console.log(`Stream processing file: ${fileName} (${fileSizeInMB.toFixed(2)} MB)`);
    console.log(`Using ${numAnalyses} analysis perspectives with ${reqPerAnalysis} requirements each`);
    
    // Extract domain information
    const domainsList = ['CRM', 'ERP', 'service cloud', 'sales cloud', 'marketing cloud', 'commerce cloud', 'call center', 
      'customer service', 'field service', 'salesforce', 'dynamics', 'sap', 'oracle', 'servicenow', 'zendesk'];
    
    // Check if any domain keywords are in the filename or project name
    // Add null/undefined checks
    const matchedKeywords = domainsList.filter(domain => 
      (fileName && typeof fileName === 'string' && domain && typeof domain === 'string' && fileName.toLowerCase().includes(domain.toLowerCase())) || 
      (projectName && typeof projectName === 'string' && domain && typeof domain === 'string' && projectName.toLowerCase().includes(domain.toLowerCase()))
    );
    
    const inferredDomain = matchedKeywords.length > 0 
      ? matchedKeywords.join(', ') 
      : "service management"; // Default to service management if no specific domain is detected
    
    // Define perspectives based on file type - similar to generateRequirementsForFile
    const perspectives = [
      {
        name: "Functional Requirements",
        focus: "core functionality and business processes that must be migrated"
      },
      {
        name: "Data Requirements",
        focus: "data structures, fields, and relationships that must be preserved"
      },
      {
        name: "Integration Requirements",
        focus: "integration points, APIs, and external system connections"
      },
      {
        name: "User Experience Requirements",
        focus: "user interfaces, workflows, and experience aspects"
      },
      {
        name: "Security & Compliance Requirements",
        focus: "security controls, access permissions, and compliance needs"
      }
    ];

    // Select perspectives based on numAnalyses
    const selectedPerspectives = perspectives.slice(0, Math.min(numAnalyses, perspectives.length));
    
    // Get file content based on file type
    let fileContent = '';
    let fileExtractSuccess = true;
    let fileExtractError = '';
    
    // Handle different file types for text extraction
    if (fileType.toLowerCase() === '.docx' || fileType.toLowerCase() === '.doc') {
      logger.info(`Processing DOCX file: ${filePath}`);
      const docxResult = await extractTextFromDocx(filePath);
      if (!docxResult.success) {
        logger.error(`Failed to extract text from DOCX: ${docxResult.error}`);
        fileExtractSuccess = false;
        fileExtractError = docxResult.error || 'Unknown error extracting DOCX text';
      } else {
        fileContent = docxResult.text;
        logger.info(`Successfully extracted ${fileContent.length} characters from DOCX`);
      }
    } else if (fileType.toLowerCase() === '.txt' || fileType.toLowerCase() === '.md') {
      logger.info(`Processing text file: ${filePath}`);
      
      // Check file size to determine approach
      const fileSizeMB = fileSize / (1024 * 1024);
      const isLargeFile = fileSizeMB > 5;
      
      // For large files, use streamMode to avoid loading the entire file
      const txtResult = await extractTextFromTxt(filePath, isLargeFile);
      
      if (!txtResult.success) {
        logger.error(`Failed to extract text from text file: ${txtResult.error}`);
        fileExtractSuccess = false;
        fileExtractError = txtResult.error || 'Unknown error reading text file';
      } else if (isLargeFile) {
        // For large files in streaming mode, we don't load the content here
        // We'll process it in chunks later
        fileContent = txtResult.preview || 'Large text file - processing in chunks';
        logger.info(`Large text file detected (${fileSizeMB.toFixed(2)} MB). Will process in streaming mode.`);
      } else {
        // For smaller files, use the content directly
        fileContent = txtResult.text;
        logger.info(`Successfully read ${fileContent.length} characters from text file`);
      }
    } else {
      // For other file types (binary, etc.), we might need specialized handling
      // but for now, we'll just proceed with the default stream processing
      logger.info(`Processing file with type ${fileType} using default method`);
    }
    
    // If text extraction failed, throw an error to be caught by the try/catch block
    if (!fileExtractSuccess) {
      throw new Error(`Failed to extract content from file: ${fileExtractError}`);
    }
    
    // Stream the file to a temporary location in chunks if needed
    // This process is more important for larger files
    const chunkSize = 5 * 1024 * 1024; // 5MB chunks for efficient processing
    const numChunks = Math.ceil(fileSize / chunkSize);
    let textChunks: string[] = [];
    
    // For smaller files, we can use the entire file as one chunk
    if (numChunks <= 1 || fileSize < 1024 * 1024) { // Under 1MB
      console.log(`Small file detected (${fileSizeInMB.toFixed(2)} MB). Processing as a single chunk.`);
      
      // If we already have file content, use it directly
      if (fileContent) {
        textChunks = [fileContent];
      }
    } else {
      console.log(`Processing large file in ${numChunks} chunks of ~5MB each`);
      
      // For txt files, implement a memory-efficient chunking approach
      if ((fileType.toLowerCase() === '.txt' || fileType.toLowerCase() === '.md') && fileSizeInMB > 5) {
        // Use streaming approach for large text files
        textChunks = await processLargeTextFileInChunks(filePath, Math.min(5, numChunks));
        console.log(`Split large text file into ${textChunks.length} chunks for processing`);
      } 
      // For other file types, just use the content we already loaded if available
      else if (fileContent) {
        // Split into reasonable chunks based on content patterns
        textChunks = splitTextIntoChunks(fileContent, 2000000); // ~2MB text chunks
        console.log(`Split content into ${textChunks.length} chunks for processing`);
      }
    }

    // Initialize an array to store all requirements
    let allRequirements: any[] = [];
    
    // Process from each perspective
    for (let i = 0; i < selectedPerspectives.length; i++) {
      const perspective = selectedPerspectives[i];
      console.log(`Processing analysis perspective ${i+1}/${selectedPerspectives.length}: ${perspective.name}`);
      
      // Get the Gemini model
      const model = genAI.getGenerativeModel({
        model: "gemini-1.5-pro",
        generationConfig,
        safetySettings,
      });

      // Create a specialized prompt for this file type and perspective
      const prompt = `
        You are a business systems analyst specializing in ${inferredDomain} systems with expertise in ${perspective.name.toLowerCase()}. 
        Your task is to generate migration requirements for a project that's moving functionality from a source system to a target system, 
        focusing specifically on ${perspective.focus}.
        
        Project context: ${projectName}
        File name: ${fileName}
        File type: ${fileType}
        Content type: ${contentType}
        Inferred domain: ${inferredDomain}
        Analysis perspective: ${perspective.name} (focusing on ${perspective.focus})
        
        ${contentType === 'workflow' ? 
          `This ${fileType} file documents workflows and business processes in a ${inferredDomain} system that must be recreated in a target system.
          
          Assume this file contains information about ${inferredDomain} workflows and processes.
          Generate detailed migration requirements focused on ${perspective.focus} aspects of these workflows.`
          
          : contentType === 'user_feedback' ? 
          `This ${fileType} file contains user feedback about a ${inferredDomain} system.
          
          Assume users are providing feedback about different aspects of the system.
          Generate requirements related to ${perspective.focus} that address specific user needs in the target system.` 
          
          : contentType === 'documentation' ? 
          `This ${fileType} file contains documentation about a ${inferredDomain} system.
          
          Assume this documentation describes various system capabilities.
          Generate requirements for ${perspective.focus} that ensure these documented capabilities are preserved in the target system.` 
          
          : contentType === 'specifications' ? 
          `This ${fileType} file contains technical specifications for a ${inferredDomain} system.
          
          Assume these specifications cover various technical aspects.
          Generate specific requirements related to ${perspective.focus} based on typical specifications for ${inferredDomain} systems.` 
          
          : `This ${fileType} file contains information related to a ${inferredDomain} system.
          
          Generate requirements focusing on ${perspective.focus} for ${inferredDomain} systems.`
        }
        
        For each requirement:
        1. Provide a concise title (3-10 words) that summarizes the requirement
        2. Provide a detailed, domain-specific requirement description of at least 150 words related to ${perspective.focus} within ${inferredDomain} functionality
        3. Classify it into one of these categories: 'functional', 'non-functional', 'security', 'performance'
        4. Assign a priority level: 'high', 'medium', or 'low'
        
        Format your response as a JSON array with exactly ${reqPerAnalysis} requirements, each with the properties 'title', 'description', 'category', and 'priority'.
        Example: [{"title": "Call Center Queue Logic", "description": "The target system must maintain the current call center queuing logic that routes cases based on SLA priority and agent skill matching... [detailed 150+ word description that thoroughly explains the requirement]", "category": "functional", "priority": "high"}, ...]
        
        Only output valid JSON with no additional text or explanations.
      `;

      try {
        // Store the prompt in a file to possibly refer back to it later
        const promptPath = path.join(tempDir, `prompt_${i}.txt`);
        fs.writeFileSync(promptPath, prompt, 'utf8');
        
        // Process each chunk (if we have chunks) for this perspective
        if (textChunks.length > 0) {
          // For each text chunk, process it separately
          for (let j = 0; j < textChunks.length; j++) {
            const chunk = textChunks[j];
            console.log(`Processing chunk ${j+1}/${textChunks.length} for perspective ${perspective.name}`);
            
            // Modify the prompt to include chunk content
            const chunkPrompt = prompt + `\n\nFile content (chunk ${j+1} of ${textChunks.length}):\n${chunk}\n`;
            
            // Generate content for this chunk
            const chunkResult = await model.generateContent(chunkPrompt);
            const chunkResponse = await chunkResult.response;
            const text = chunkResponse.text();
            
            // Process text, extract requirements, and add to allRequirements
            try {
              // Parse JSON from response
              const jsonMatch = text.match(/\[[\s\S]*\]/);
              if (jsonMatch) {
                const jsonText = jsonMatch[0];
                const parsedResponse = JSON.parse(jsonText);
                
                // Transform the response to match the expected format
                const chunkRequirements = parsedResponse.map((item: any) => ({
                  title: item.title || `${perspective.name} Requirement (Chunk ${j+1})`,
                  description: item.description || item.text,
                  category: item.category,
                  priority: item.priority
                }));
                
                allRequirements = [...allRequirements, ...chunkRequirements];
                console.log(`Added ${chunkRequirements.length} requirements from chunk ${j+1} of perspective ${perspective.name}`);
              }
            } catch (chunkParseError) {
              console.error(`Error parsing Gemini response for chunk ${j+1} of perspective ${perspective.name}:`, chunkParseError);
              console.error("Raw response:", text);
              // Continue with other chunks even if one fails
            }
          }
        } else {
          // If no chunks were created (possibly due to extraction issues),
          // just process with the original prompt
          const result = await model.generateContent(prompt);
          const response = await result.response;
          const text = response.text();
          
          // Parse the JSON response
          try {
            // Extract just the JSON part from the response
            const jsonMatch = text.match(/\[[\s\S]*\]/);
            if (jsonMatch) {
              const jsonText = jsonMatch[0];
              const parsedResponse = JSON.parse(jsonText);
              
              // Transform the response to match the expected format
              const requirements = parsedResponse.map((item: any) => ({
                title: item.title || `${perspective.name} Requirement`,
                description: item.description || item.text, // Use description or fall back to text field for backward compatibility
                category: item.category,
                priority: item.priority
              }));
              
              allRequirements = [...allRequirements, ...requirements];
              console.log(`Added ${requirements.length} requirements from perspective ${perspective.name}`);
            } else {
              // If no JSON array was found, try parsing the whole response
              const parsedResponse = JSON.parse(text);
              
              // Transform the response to match the expected format
              const requirements = parsedResponse.map((item: any) => ({
                title: item.title || `${perspective.name} Requirement`,
                description: item.description || item.text, // Use description or fall back to text field for backward compatibility
                category: item.category,
                priority: item.priority
              }));
              
              allRequirements = [...allRequirements, ...requirements];
              console.log(`Added ${requirements.length} requirements from perspective ${perspective.name}`);
            }
          } catch (parseError) {
            console.error(`Error parsing Gemini response for perspective ${perspective.name}:`, parseError);
            console.error("Raw response:", text);
            // Continue with other perspectives even if one fails
          }
      } catch (perspectiveError) {
        console.error(`Error processing perspective ${perspective.name}:`, perspectiveError);
        // Continue with other perspectives even if one fails
      }
      
      // Small pause between analysis passes to avoid rate limiting
      if (i < selectedPerspectives.length - 1) {
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }
    
    // Remove any duplicate requirements (comparing by title or description)
    const uniqueRequirements = allRequirements.filter((req, index, self) => {
      // Handle both new format (title/description) and legacy format (text)
      const reqText = req.description || req.text;
      return index === self.findIndex((r) => {
        const rText = r.description || r.text;
        return reqText === rText;
      });
    });
    
    console.log(`Extracted ${uniqueRequirements.length} unique requirements from ${selectedPerspectives.length} analysis perspectives`);
    
    // If we have an inputDataId, we could potentially process references here
    // Similar to how it's done in processTextFile or streamProcessPdfText
    
    return uniqueRequirements;
  } catch (error) {
    console.error("Error in stream file processor:", error);
    throw error;
  } finally {
    // Clean up temporary directory
    try {
      if (fs.existsSync(tempDir)) {
        // Remove all files in the temp directory
        const files = fs.readdirSync(tempDir);
        for (const file of files) {
          await unlink(path.join(tempDir, file));
        }
        // Remove the directory
        fs.rmdirSync(tempDir);
      }
    } catch (cleanupError) {
      console.error("Error cleaning up temporary directory:", cleanupError);
    }
  }
}

/**
 * Stream a file to disk, which is memory-efficient for large files
 * @param sourcePath Source file path
 * @param destPath Destination file path
 */
async function streamFileToDisk(sourcePath: string, destPath: string): Promise<void> {
  return pipeline(
    createReadStream(sourcePath),
    createWriteStream(destPath)
  );
}

/**
 * Process a large text file by reading it in chunks
 * This is memory-efficient for very large files
 * 
 * @param filePath Path to the text file
 * @param maxChunks Maximum number of chunks to extract
 * @returns Array of text chunks
 */
async function processLargeTextFileInChunks(filePath: string, maxChunks: number = 5): Promise<string[]> {
  try {
    logger.info(`Processing large text file in chunks: ${filePath}`);
    
    // Get file stats
    const stats = fs.statSync(filePath);
    
    // Calculate chunk size to evenly divide the file
    const chunkSizeBytes = Math.ceil(stats.size / maxChunks);
    
    // Prepare buffer and result array
    const buffer = Buffer.alloc(chunkSizeBytes);
    const fd = fs.openSync(filePath, 'r');
    const chunks: string[] = [];
    
    try {
      // Read evenly spaced chunks from the file
      for (let i = 0; i < maxChunks; i++) {
        // Calculate position to read from
        const position = i * chunkSizeBytes;
        
        // Skip if we're past the end of file
        if (position >= stats.size) break;
        
        // Read a chunk
        const bytesRead = fs.readSync(fd, buffer, 0, chunkSizeBytes, position);
        
        if (bytesRead > 0) {
          const chunkContent = buffer.slice(0, bytesRead).toString('utf8');
          
          // Clean up chunk boundaries to avoid cutting in the middle of words
          let cleanedChunk = chunkContent;
          
          // Find sentence boundaries for cleaner chunks
          if (i > 0) {
            // For all chunks except first, trim beginning to a sentence start
            const sentenceStart = cleanedChunk.match(/[.!?]\s+[A-Z]/);
            if (sentenceStart && sentenceStart.index && sentenceStart.index < 1000) {
              cleanedChunk = cleanedChunk.substring(sentenceStart.index + 2);
            }
          }
          
          if (i < maxChunks - 1 && position + bytesRead < stats.size) {
            // For all chunks except last, trim end to a sentence end
            const reversedChunk = cleanedChunk.split('').reverse().join('');
            const sentenceEnd = reversedChunk.match(/[A-Z]\s+[.!?]/);
            if (sentenceEnd && sentenceEnd.index && sentenceEnd.index < 1000) {
              cleanedChunk = cleanedChunk.substring(0, cleanedChunk.length - (sentenceEnd.index + 2));
            }
          }
          
          chunks.push(cleanedChunk);
        }
      }
    } finally {
      fs.closeSync(fd);
    }
    
    logger.info(`Successfully split text file into ${chunks.length} chunks`);
    return chunks;
  } catch (error) {
    logger.error(`Error processing large text file in chunks: ${error}`);
    throw error;
  }
}

/**
 * Split text content into semantic chunks
 * 
 * @param text The text content to split
 * @param targetChunkSize Target size for each chunk in characters
 * @returns Array of text chunks
 */
function splitTextIntoChunks(text: string, targetChunkSize: number = 2000000): string[] {
  // If text is smaller than target size, return it as a single chunk
  if (text.length <= targetChunkSize) {
    return [text];
  }
  
  const chunks: string[] = [];
  
  // First try to split by double newlines (paragraphs)
  const paragraphs = text.split(/\n\s*\n/);
  
  let currentChunk = '';
  
  for (const paragraph of paragraphs) {
    // If adding this paragraph would exceed target size, start a new chunk
    if (currentChunk.length + paragraph.length > targetChunkSize && currentChunk.length > 0) {
      chunks.push(currentChunk);
      currentChunk = paragraph;
    } else {
      currentChunk += (currentChunk ? '\n\n' : '') + paragraph;
    }
  }
  
  // Don't forget the last chunk
  if (currentChunk) {
    chunks.push(currentChunk);
  }
  
  // If we couldn't split into enough chunks using paragraphs,
  // we'll split the largest chunks further
  if (chunks.length === 1 && text.length > targetChunkSize * 2) {
    // Try to split by sentences
    return splitLargeChunkBySentences(text, targetChunkSize);
  }
  
  return chunks;
}

/**
 * Split a very large chunk by sentences when paragraph splitting isn't sufficient
 * 
 * @param text The text content to split
 * @param targetChunkSize Target size for each chunk in characters
 * @returns Array of text chunks
 */
function splitLargeChunkBySentences(text: string, targetChunkSize: number): string[] {
  // Use a regex pattern to find sentence boundaries
  const sentencePattern = /[.!?]\s+[A-Z]/g;
  const chunks: string[] = [];
  
  let lastEnd = 0;
  let currentChunkStart = 0;
  
  // Find all sentence boundaries
  let match;
  while ((match = sentencePattern.exec(text)) !== null) {
    const sentenceEnd = match.index + 1; // Include the punctuation but not the space or next capital
    
    // If we've accumulated enough text for a chunk, add it
    if (sentenceEnd - currentChunkStart > targetChunkSize) {
      // If we have a valid last end point, use it
      if (lastEnd > currentChunkStart) {
        chunks.push(text.substring(currentChunkStart, lastEnd + 1));
        currentChunkStart = lastEnd + 2; // Skip the space after punctuation
      } else {
        // If no good break point, just cut at target size
        chunks.push(text.substring(currentChunkStart, currentChunkStart + targetChunkSize));
        currentChunkStart += targetChunkSize;
      }
    }
    
    lastEnd = sentenceEnd;
  }
  
  // Add the remaining text
  if (currentChunkStart < text.length) {
    chunks.push(text.substring(currentChunkStart));
  }
  
  // If we still couldn't split effectively, just divide evenly
  if (chunks.length === 0) {
    const numChunks = Math.ceil(text.length / targetChunkSize);
    for (let i = 0; i < numChunks; i++) {
      const start = i * targetChunkSize;
      const end = Math.min(start + targetChunkSize, text.length);
      chunks.push(text.substring(start, end));
    }
  }
  
  return chunks;
}

/**
 * Utility function to generate a unique ID for a nanoid-like ID
 * @param length Length of the ID
 * @returns Random ID string
 */
function generateUniqueId(length: number = 8): string {
  const chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789';
  let id = '';
  for (let i = 0; i < length; i++) {
    id += chars.charAt(Math.floor(Math.random() * chars.length));
  }
  return id;
}